import os
# [ì¤‘ìš”] Mac ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë°˜ë“œì‹œ import torch ì „ì—)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from peft import PeftModel

# ê¸°ì¡´ ëª¨ë“ˆ import ìœ ì§€
from models.intent_classifier import IntentClassifier
from models.translator import Translator
from models.normalizer import Normalizer
from models.chat_model import ChatModel
from models.rag.personal_rag import PersonalRAG
from models.rag.personal_response import PersonalResponse
from models.rag.behavior_detector import BehaviorDetector
from models.rag.decision_model import DecisionModel

class ModelContainer:
    _instance = None

    def __init__(self):
        print("ğŸ“¥ AI ëª¨ë¸ ë¡œë”© ì‹œì‘... (ìµœì´ˆ 1íšŒ)")
        
        # [ì¤‘ìš”] ë§¥ë¶(MPS) ê°€ì† ì§€ì› ì¶”ê°€
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"  # ë§¥ë¶ M1/M2/M3 ì „ìš© ê°€ì†
        else:
            self.device = "cpu"
            
        print(f"ğŸš€ ì‹¤í–‰ ì¥ì¹˜: {self.device}")

        self.intent_classifier = IntentClassifier()
        self.chat_model = ChatModel()
        self.translator = Translator()
        self.normalizer = Normalizer()

        self.rag = PersonalRAG()
        self.personal_response = PersonalResponse()
        self.behavior_detector = BehaviorDetector()
        self.decision_model = DecisionModel()

        print("ğŸ‘‚ Whisper(STT) ëª¨ë¸ ë¡œë”© ì¤‘...")

        BASE_MODEL_ID = "openai/whisper-small"
        ADAPTER_PATH = "./models/whisper-finetuned-v1"

        # [ìˆ˜ì •ë¨] Processorì™€ Model ë³€ìˆ˜ ë¶„ë¦¬
        self.processor = WhisperProcessor.from_pretrained(BASE_MODEL_ID, language="Korean", task="transcribe")
        
        # device_mapì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì¥ì¹˜ í• ë‹¹
        base_hf_model = WhisperForConditionalGeneration.from_pretrained(
            BASE_MODEL_ID, 
            device_map=self.device
        )

        if os.path.exists(ADAPTER_PATH):
            self.stt_model = PeftModel.from_pretrained(base_hf_model, ADAPTER_PATH)
            print("âœ… LoRA ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ!")
        else:
            print(f"âš ï¸ ì–´ëŒ‘í„° ì—†ìŒ. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {ADAPTER_PATH}")
            self.stt_model = base_hf_model

        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.stt_model.eval()

        self.waiting_for_decision = False
        self.pending_task = None
        print("âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance