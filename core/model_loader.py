import os
# [ì¤‘ìš”] Mac ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë°˜ë“œì‹œ import torch ì „ì—)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from peft import PeftModel

# ê¸°ì¡´ ëª¨ë“ˆ import ìœ ì§€
from models.intent_classifier import IntentClassifier
from models.translator import Translator
from models.normalizer import Normalizer
from models.chat_model import ChatModel
from models.rag.personal_rag import PersonalRAG
from models.rag.personal_response import PersonalResponse
from models.rag.behavior_detector import BehaviorDetector
from models.rag.decision_model import DecisionModel

class ModelContainer:
    _instance = None

    def __init__(self):
        print("ğŸ“¥ AI ëª¨ë¸ ë¡œë”© ì‹œì‘... (Medium + Small Dual Setup)")
        
        # ì¥ì¹˜ ì„¤ì •
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"
        print(f"ğŸš€ ì‹¤í–‰ ì¥ì¹˜: {self.device}")

        # 1. ê¸°íƒ€ ëª¨ë¸ ë¡œë“œ
        self.intent_classifier = IntentClassifier()
        self.chat_model = ChatModel()
        self.translator = Translator()
        self.normalizer = Normalizer()

        self.rag = PersonalRAG()
        self.personal_response = PersonalResponse()
        self.behavior_detector = BehaviorDetector()
        self.decision_model = DecisionModel()

        # ---------------------------------------------------------
        # [í•µì‹¬ ìˆ˜ì •] 1ì°¨(Medium) & 2ì°¨(Small+LoRA) ëª¨ë¸ ë¶„ë¦¬ ë¡œë”©
        # ---------------------------------------------------------
        print("ğŸ‘‚ Whisper ëª¨ë¸ 2ì¢…(Medium, Small) ë¡œë”© ì¤‘...")

        ID_MEDIUM = "openai/whisper-medium"
        ID_SMALL = "openai/whisper-small"
        ADAPTER_PATH = "./models/whisper-finetuned-v1"

        # ProcessorëŠ” Medium ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ë§Œ ìƒì„± (Smallê³¼ í† í¬ë‚˜ì´ì € í˜¸í™˜ë¨)
        self.processor = WhisperProcessor.from_pretrained(ID_MEDIUM)

        # [1ì°¨] Medium ëª¨ë¸ (ìˆœì •)
        print(f"   - Loading 1st Stage: {ID_MEDIUM}...")
        self.stt_model_medium = WhisperForConditionalGeneration.from_pretrained(
            ID_MEDIUM, 
            device_map=self.device
        )
        self.stt_model_medium.eval()

        # [2ì°¨] Small ëª¨ë¸ + LoRA (íŠ¹í™”)
        print(f"   - Loading 2nd Stage: {ID_SMALL} + LoRA...")
        base_small = WhisperForConditionalGeneration.from_pretrained(
            ID_SMALL, 
            device_map=self.device
        )

        if os.path.exists(ADAPTER_PATH):
            self.stt_model_small_lora = PeftModel.from_pretrained(base_small, ADAPTER_PATH)
            print("     âœ… Small ëª¨ë¸ì— LoRA ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!")
        else:
            print(f"     âš ï¸ ì–´ëŒ‘í„° ê²½ë¡œ ì—†ìŒ. Small ìˆœì •ìœ¼ë¡œ ë™ì‘í•¨.")
            self.stt_model_small_lora = base_small
        
        self.stt_model_small_lora.eval()

        print("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance