import uvicorn
import socketio
import asyncio
import sys
import os
import base64
import numpy as np
import cv2
from typing import Dict, Any, Optional
import base64
import uuid
import asyncio
import librosa
import torch

from pydub import AudioSegment
from pydub.effects import normalize as pydub_normalize

# ì»¤ìŠ¤í…€ ëª¨ë“ˆ
from core.router import Router
from core.model_loader import ModelContainer

try:
    import audioop_lts
    sys.modules["audioop"] = audioop_lts
except ImportError:
    pass

# í™˜ê²½ ì„¤ì •
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
PORT = int(os.getenv("PORT", 3000))
HEALTH_KEYWORDS = ["ë³‘ì›", "ì§„ë£Œ", "ì˜ì‚¬", "ê°„í˜¸ì‚¬", "ì¦ìƒ", "ì•„íŒŒ", "ì˜ˆì•½", "ìƒë‹´", "ê±´ê°•", "ìˆ˜ìˆ ", "ê²€ì§„", "ì•½", "ë³µìš©"]

# ----------------------------------------------------------------
# 1. ì „ì—­ ëª¨ë¸ ë¡œë”© (Singleton)
# ----------------------------------------------------------------
# ì„œë²„ ì‹œì‘ ì‹œ ë”± í•œ ë²ˆë§Œ ë¬´ê±°ìš´ ëª¨ë¸ë“¤ì„ ë¡œë”©í•©ë‹ˆë‹¤.
global_models = ModelContainer.get_instance()

# ----------------------------------------------------------------
# 2. ì„œë²„ ë° ì„¸ì…˜ ì„¤ì •
# ----------------------------------------------------------------
sio = socketio.AsyncServer(async_mode='asgi', cors_allowed_origins='*')
app = socketio.ASGIApp(sio)

sessions: Dict[str, Router] = {}

# ----------------------------------------------------------------
# 3. í—¬í¼ í•¨ìˆ˜ (ì¤‘ë³µ ë¡œì§ ì œê±°)
# ----------------------------------------------------------------
def format_response_payload(response_data: Any) -> Dict[str, Any]:
    """
    Routerì˜ ë°˜í™˜ê°’ì„ ë¶„ì„í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ê·œê²©(JSON)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    data, meta = None, None

    # (ë°ì´í„°, ë©”íƒ€ë°ì´í„°) íŠœí”Œ í˜•íƒœì¸ì§€ í™•ì¸
    if isinstance(response_data, (tuple, list)) and len(response_data) == 2:
        data, meta = response_data
    else:
        data = response_data

    # set íƒ€ì…ì€ JSON ì§ë ¬í™” ë¶ˆê°€í•˜ë¯€ë¡œ listë¡œ ë³€í™˜
    if isinstance(data, set):
        data = list(data)

    # ë©”íƒ€ë°ì´í„° ìœ ë¬´ì— ë”°ë¼ ì‘ë‹µ íƒ€ì… ê²°ì •
    msg_type = "confirm" if meta else "simple"
    
    return {
        "text": data,
        "type": msg_type,
        "meta": meta # í•„ìš”í•˜ë‹¤ë©´ ë©”íƒ€ë°ì´í„°ë„ í•¨ê»˜ ì „ì†¡
    }

def decode_image(base64_string: str) -> Optional[np.ndarray]:
    """
    Base64 ë¬¸ìì—´ì„ OpenCV ì´ë¯¸ì§€ ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    try:
        if ',' in base64_string:
            _, base64_data = base64_string.split(',', 1)
        else:
            base64_data = base64_string

        img_data = base64.b64decode(base64_data)
        nparr = np.frombuffer(img_data, np.uint8)
        return cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    except Exception as e:
        print(f"ğŸ–¼ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        return None

def get_or_create_router(sid: str) -> Optional[Router]:
    """
    ì„¸ì…˜ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ì¬ìƒì„±í•©ë‹ˆë‹¤.
    """
    if sid not in sessions:
        try:
            # [ì¤‘ìš”] ì¬ìƒì„± ì‹œì—ë„ ë°˜ë“œì‹œ ì „ì—­ ëª¨ë¸ì„ ì£¼ì…í•´ì•¼ í•©ë‹ˆë‹¤.
            sessions[sid] = Router(models=global_models)
        except Exception as e:
            print(f"ğŸš¨ Router ì¬ìƒì„± ì‹¤íŒ¨ ({sid}): {e}")
            return None
    return sessions[sid]


# ----------------------------------------------------------------
# 4. ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
# ----------------------------------------------------------------

@sio.event
async def connect(sid, environ):
    print(f"âœ… í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨: {sid}")
    get_or_create_router(sid)

@sio.event
async def disconnect(sid):
    print(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€: {sid}")
    sessions.pop(sid, None) # ì•ˆì „í•œ ì‚­ì œ

@sio.on('command')
async def handle_command(sid, data):
    """ ì•± -> ì„œë²„: í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡ """
    print(f"ğŸ“© ìˆ˜ì‹  ({sid}): {data}")
    user_text = data.get('text', '')
    
    router = get_or_create_router(sid)
    if not router:
        await sio.emit('command-response', {"text": "ì„œë²„ ì´ˆê¸°í™” ì˜¤ë¥˜", "type": "error"}, to=sid)
        return

    try:
        # AI ì¶”ë¡  (ë¹„ë™ê¸° ìŠ¤ë ˆë“œ ì‹¤í–‰)
        response_data = await asyncio.to_thread(router.handle, user_text)
        
        # ì‘ë‹µ í¬ë§·íŒ… ë° ì „ì†¡
        payload = format_response_payload(response_data)
        await sio.emit('command-response', payload, to=sid)
        print(f"ğŸ“¤ ì „ì†¡: {payload}")

    except Exception as e:
        print(f"ğŸš¨ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        await sio.emit('command-response', {"text": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", "type": "error"}, to=sid)


@sio.on('action-confirm')
async def handle_action_confirm(sid, data):
    """ ì•± -> ì„œë²„: [ë„¤] ë²„íŠ¼ í´ë¦­ """
    print(f"ğŸ”˜ ë²„íŠ¼ í´ë¦­ ìˆ˜ì‹  (YES): {data}")
    
    router = get_or_create_router(sid)
    if not router:
        return

    try:
        # Decision ë¡œì§ ìˆ˜í–‰ ("ë„¤"ë¼ëŠ” í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬)
        response_data = await asyncio.to_thread(router.handle, "ë„¤")
        
        # ì‘ë‹µ í¬ë§·íŒ… ë° ì „ì†¡ (handle_commandì™€ ë™ì¼ ë¡œì§ ì‚¬ìš©)
        payload = format_response_payload(response_data)
        await sio.emit('command-response', payload, to=sid)
        print(f"ğŸ“¤ ì‹¤í–‰ ì™„ë£Œ ì‘ë‹µ ì „ì†¡: {payload}")

    except Exception as e:
        print(f"ğŸš¨ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        await sio.emit('command-response', {"text": "ì‹¤í–‰ ì˜¤ë¥˜ ë°œìƒ", "type": "error"}, to=sid)

@sio.on('audio-upload')
async def handle_audio_upload(sid, data):
    """
    ì•± -> ì„œë²„: ìˆ˜ì‹  -> ì „ì²˜ë¦¬ -> [í•˜ì´ë¸Œë¦¬ë“œ STT] -> Router -> ì‘ë‹µ
    """
    print(f"ğŸ¤ ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹  ({sid})")
    
    router = get_or_create_router(sid)
    if not router:
        return

    raw_filename = None
    processed_filename = None

    try:
        # -------------------------------------------------------
        # 1. ë°ì´í„° íŒŒì‹± ë° íŒŒì¼ ì €ì¥
        # -------------------------------------------------------
        b64_string = data.get('audioData')
        file_ext = data.get('format', 'm4a')
        user_id = data.get('userId', 'unknown')

        audio_bytes = base64.b64decode(b64_string)
        
        if not os.path.exists('uploads'):
            os.makedirs('uploads')
            
        raw_filename = f"uploads/{user_id}_{uuid.uuid4()}.{file_ext}"
        with open(raw_filename, "wb") as f:
            f.write(audio_bytes)
            
        print(f"ğŸ’¾ ì›ë³¸ ì €ì¥ ì™„ë£Œ: {raw_filename}")

        # -------------------------------------------------------
        # 2. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (Preprocessing)
        # -------------------------------------------------------
        def preprocess_audio():
            print("ğŸ›ï¸ ì „ì²˜ë¦¬: Resample(16k) & Normalize")
            audio = AudioSegment.from_file(raw_filename, format=file_ext)
            audio = audio.set_channels(1)       # Mono
            audio = audio.set_frame_rate(16000) # 16kHz
            audio = pydub_normalize(audio)      # Volume Maximize
            
            new_filename = raw_filename.replace(f".{file_ext}", "_processed.wav")
            audio.export(new_filename, format="wav")
            return new_filename

        processed_filename = await asyncio.to_thread(preprocess_audio)
        print(f"âœ¨ ì „ì²˜ë¦¬ ì™„ë£Œ: {processed_filename}")

        # -------------------------------------------------------
        # 3. [1ì°¨] ì¼ë°˜ Whisper ì¸ì‹ (í™•ì‹ ë„ ì²´í¬)
        # -------------------------------------------------------
        print("ğŸ‘‚ [1ë‹¨ê³„] ì¼ë°˜ ëª¨ë¸ ì¸ì‹ ì¤‘...")
        models = global_models # ModelContainer ì¸ìŠ¤í„´ìŠ¤

        def transcribe_std():
            # OpenAI ëª¨ë¸ì€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ë©° 'segments' ì•ˆì— 'avg_logprob'ê°€ ìˆìŒ
            result = models.stt_model.transcribe(
                processed_filename, 
                language="ko", 
                fp16=False,
                beam_size=5,
                initial_prompt="ê±´ê°• ìƒë‹´, ëª¸ ìƒíƒœ, í—ˆì•½ ì²´ì§ˆ, ë³‘ì› ì§„ë£Œì— ëŒ€í•œ ëŒ€í™”ì…ë‹ˆë‹¤."
            )
            
            text = result['text'].strip()
            # í™•ì‹ ë„(Log Probability) ì¶”ì¶œ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í™•ì‹¤, -1 ì´í•˜ë©´ ë¶ˆí™•ì‹¤)
            # segmentsê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ì²˜ë¦¬
            score = -10.0
            if result.get('segments'):
                score = result['segments'][0].get('avg_logprob', -10.0)
            
            return text, score

        text_std, score_std = await asyncio.to_thread(transcribe_std)
        print(f"ğŸ—£ï¸ [1ì°¨ ê²°ê³¼] '{text_std}' (í™•ì‹ ë„: {score_std:.2f})")

        # -------------------------------------------------------
        # 4. [íŒë‹¨] êµ¬ìŒì¥ì•  ëª¨ë¸ ê°€ë™ ì—¬ë¶€ ê²°ì •
        # -------------------------------------------------------
        use_dys_model = False
        
        # ì¡°ê±´ A: í™•ì‹ ë„ê°€ ë‚®ìŒ (AIê°€ ì˜ ëª»ì•Œì•„ë“¤ìŒ) -> -0.7 ê¸°ì¤€ (ì¡°ì • ê°€ëŠ¥)
        if score_std < -0.7:
            use_dys_model = True
            print("ğŸ“‰ í™•ì‹ ë„ ë‚®ìŒ -> íŠ¹í™” ëª¨ë¸ ì „í™˜")
            
        # ì¡°ê±´ B: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ (ì˜¤ì¸ì‹ ê°€ëŠ¥ì„± ë†’ìŒ)
        elif len(text_std) < 3:
            use_dys_model = True
            print("ğŸ“‰ í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ -> íŠ¹í™” ëª¨ë¸ ì „í™˜")

        # ì¡°ê±´ C: [Override] ë³‘ì› í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¼ë°˜ ëª¨ë¸ ì‹ ë¢°
        # (êµ¬ìŒì¥ì•  ëª¨ë¸ì€ 'ë¹„íƒ€ë¯¼', 'ì—°í•„' ë“± ìƒí™œ ìš©ì–´ì— í¸í–¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
        for kw in HEALTH_KEYWORDS:
            if kw in text_std:
                use_dys_model = False
                print(f"ğŸ¥ ë³‘ì› í‚¤ì›Œë“œ('{kw}') ê°ì§€ -> ì¼ë°˜ ëª¨ë¸ ìœ ì§€")
                break

        final_text = text_std

        # -------------------------------------------------------
        # 5. [2ì°¨] êµ¬ìŒì¥ì•  íŠ¹í™” ëª¨ë¸ (í•„ìš” ì‹œ ì‹¤í–‰)
        # -------------------------------------------------------
        if use_dys_model:
            print("ğŸš€ [2ë‹¨ê³„] êµ¬ìŒì¥ì•  íŠ¹í™” ëª¨ë¸ ê°€ë™")

            def transcribe_dys():
                # Librosaë¡œ ë¡œë“œ (WhisperProcessor ì…ë ¥ìš©)
                audio_array, _ = librosa.load(processed_filename, sr=16000)
                
                # Processor ì „ì²˜ë¦¬
                inputs = models.dys_processor(
                    audio_array, 
                    sampling_rate=16000, 
                    return_tensors="pt"
                ).input_features.to(models.device)

                # ì¶”ë¡ 
                with torch.no_grad():
                    generated_ids = models.dys_model.generate(inputs, language="korean")

                # ë””ì½”ë”©
                transcription = models.dys_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                return transcription.strip()

            text_dys = await asyncio.to_thread(transcribe_dys)
            print(f"ğŸ—£ï¸ [2ì°¨ ê²°ê³¼] '{text_dys}'")

            # íŠ¹í™” ëª¨ë¸ ê²°ê³¼ê°€ ìœ ì˜ë¯¸í•˜ë©´ ì±„íƒ
            if text_dys:
                final_text = text_dys

        # -------------------------------------------------------
        # 6. [í›„ì²˜ë¦¬] í…ìŠ¤íŠ¸ êµì • (Post-processing)
        # -------------------------------------------------------
        # "í™”ì•½" -> "í—ˆì•½" ê°•ì œ ì¹˜í™˜
        if "í™”ì•½" in final_text:
            final_text = final_text.replace("í™”ì•½", "í—ˆì•½")
            print("ğŸ”§ í…ìŠ¤íŠ¸ êµì •: í™”ì•½ -> í—ˆì•½")

        print(f"âœ… ìµœì¢… í™•ì •: \"{final_text}\"")

        # -------------------------------------------------------
        # 7. ì‘ë‹µ ì²˜ë¦¬
        # -------------------------------------------------------
        if not final_text:
            await sio.emit('command-response', {"text": "ì˜ ë“£ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”.", "type": "simple"}, to=sid)
            return

        # ì•±ì— ë‚´ ë§ ë¨¼ì € ë„ìš°ê¸°
        await sio.emit('user-speech', {'text': final_text}, to=sid)

        # Router ì‹¤í–‰
        response_data = await asyncio.to_thread(router.handle, final_text)
        
        # ìµœì¢… ì‘ë‹µ ì „ì†¡
        payload = format_response_payload(response_data)
        await sio.emit('command-response', payload, to=sid)
        print(f"ğŸ“¤ ì‘ë‹µ ì „ì†¡: {payload}")

    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc() # ìƒì„¸ ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
        await sio.emit('command-response', {"text": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "type": "error"}, to=sid)
    
    finally:
        # íŒŒì¼ ì •ë¦¬
        try:
            if raw_filename and os.path.exists(raw_filename):
                os.remove(raw_filename)
            if processed_filename and os.path.exists(processed_filename):
                os.remove(processed_filename)
        except Exception:
            pass

@sio.on('identify-face')
async def handle_identify_face(sid, base64_image):
    """ Expo ì•±ì˜ 'identify-face' ì´ë²¤íŠ¸ """
    print(f"ğŸ“¸ {sid} ì´ë¯¸ì§€ ìˆ˜ì‹  ({len(base64_image)} bytes)")

    try:
        # ì´ë¯¸ì§€ ë””ì½”ë”© í—¬í¼ ì‚¬ìš©
        img = await asyncio.to_thread(decode_image, base64_image)
        
        if img is None:
            await sio.emit('auth-fail', {"reason": "image_decode_error"}, to=sid)
            return

        # ... (ì–¼êµ´ ì¸ì‹ ë¡œì§ ì‹œë®¬ë ˆì´ì…˜) ...
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ img ë³€ìˆ˜ë¥¼ face_recognition ëª¨ë¸ì— ë„˜ê¹ë‹ˆë‹¤.
        await asyncio.sleep(0.5) 
        user = {"id": "p123", "name": "ê¹€ë¸”ë¼"}

        await sio.emit('auth-success', user, to=sid)
        print(f"âœ… ì¸ì¦ ì„±ê³µ: {user['name']}")

    except Exception as e:
        print(f"ğŸš¨ ì¸ì¦ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        await sio.emit('auth-fail', to=sid)

if __name__ == "__main__":
    print(f"ğŸš€ AI Router ì„œë²„ ì‹œì‘ (Port: {PORT})")
    uvicorn.run(app, host="0.0.0.0", port=PORT)